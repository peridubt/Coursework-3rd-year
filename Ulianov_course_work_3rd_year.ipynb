{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Импорт библиотек"
   ],
   "metadata": {
    "id": "8ApPJ6x5wopM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.nn import HuberLoss\n",
    "!pip install osmnx"
   ],
   "metadata": {
    "id": "_mdpqm6e0VuK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a305dd1b-9021-4a92-cbca-83f13decc5e2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting osmnx\n",
      "  Downloading osmnx-2.0.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: geopandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from osmnx) (1.0.1)\n",
      "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.11/dist-packages (from osmnx) (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from osmnx) (2.0.2)\n",
      "Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.11/dist-packages (from osmnx) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.27 in /usr/local/lib/python3.11/dist-packages (from osmnx) (2.32.3)\n",
      "Requirement already satisfied: shapely>=2.0 in /usr/local/lib/python3.11/dist-packages (from osmnx) (2.0.7)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from geopandas>=1.0->osmnx) (0.10.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from geopandas>=1.0->osmnx) (24.2)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from geopandas>=1.0->osmnx) (3.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->osmnx) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->osmnx) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->osmnx) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27->osmnx) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27->osmnx) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27->osmnx) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27->osmnx) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4->osmnx) (1.17.0)\n",
      "Downloading osmnx-2.0.1-py3-none-any.whl (99 kB)\n",
      "\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/99.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m99.6/99.6 kB\u001B[0m \u001B[31m6.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: osmnx\n",
      "Successfully installed osmnx-2.0.1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install haversine"
   ],
   "metadata": {
    "id": "EjCzkPrX0XzR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6f1d1c37-6d1c-4e62-c10c-de433f5dd0ed"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting haversine\n",
      "  Downloading haversine-2.9.0-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Downloading haversine-2.9.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Installing collected packages: haversine\n",
      "Successfully installed haversine-2.9.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_97UifVTjeQl",
    "ExecuteTime": {
     "end_time": "2025-04-01T16:43:59.032569Z",
     "start_time": "2025-04-01T16:43:34.553877Z"
    }
   },
   "source": [
    "import torch  # pytorch\n",
    "import json  # чтение json файла\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim  # регуляризация модели\n",
    "from torch.nn.utils.rnn import pad_sequence  # выраванивание последовательностей\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
    "from sklearn.model_selection import train_test_split  # разделение выборки на test/train\n",
    "from sklearn.preprocessing import MinMaxScaler  # нормализация данных\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import osmnx as ox  # библиотека для работы с OSM\n",
    "import networkx as nx  # для работы с графами местностей\n",
    "from geopy.distance import geodesic as gd\n",
    "\n",
    "from haversine import haversine, Unit  # вычисление расстояний между точками кординат\n",
    "from scipy.interpolate import interp1d  # интерполяция маршрутов\n",
    "\n",
    "import os\n",
    "import folium  # для построения html-запросов через leafnet и вывода маршрутов\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error  # метрики для модели"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Рекуррентные нейронные сети"
   ],
   "metadata": {
    "id": "isXu_940_Hl9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Класс модели LSTM"
   ],
   "metadata": {
    "id": "iqxMaMolz8-F"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Слои LSTM являются двунаправленными, т.е. они могут работать как с данными из прошлого, так и с предсказанными, отсюда для линейного слоя размер скрытого слоя увеличен в 2 раза."
   ],
   "metadata": {
    "id": "7JYK6an-wqJc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # проходим через LSTM\n",
    "        out, _ = self.lstm(x)\n",
    "        # проходим через линейный слой\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ],
   "metadata": {
    "id": "0d-8cn4djhzk",
    "ExecuteTime": {
     "end_time": "2025-04-01T16:44:06.545001Z",
     "start_time": "2025-04-01T16:44:06.540206Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T18:05:56.567648Z",
     "start_time": "2025-04-01T18:05:56.556428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class EnhancedLSTM(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=256, output_size=2, num_layers=3):\n",
    "        super(EnhancedLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Улучшенная LSTM с:\n",
    "        # - Больше слоев\n",
    "        # - Layer normalization\n",
    "        # - Dropout между слоями\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.3 if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        # Layer normalization для стабилизации LSTM выходов\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Дополнительные полносвязные слои с residual connection\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Инициализация весов\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "                # Устанавливаем forget gate bias в 1\n",
    "                n = param.size(0)\n",
    "                param.data[(n // 4):(n // 2)].fill_(1)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM layers\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.ln1(out)\n",
    "\n",
    "        # Первый полносвязный слой с residual connection\n",
    "        residual = out\n",
    "        out = self.fc1(out)\n",
    "        out = self.ln2(out)\n",
    "        out = F.leaky_relu(out, 0.1)\n",
    "        out = self.dropout(out)\n",
    "        out = out + residual[:, :, :self.hidden_size]  # residual connection\n",
    "\n",
    "        # Final output layer\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Инференс для одного маршрута\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            if isinstance(x, (list, np.ndarray)):\n",
    "                x = torch.FloatTensor(x).unsqueeze(0)  # add batch dim\n",
    "            return self.forward(x).squeeze(0).cpu().numpy()"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Загрузка обученной модели (опционально)"
   ],
   "metadata": {
    "id": "uOdXM18J3Ijx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Можно пропустить этап с обучением и сразу загрузить обученную модель (lstm_model.pth) с установившимися параметрами и перейти к шагу с [примером](https://colab.research.google.com/drive/1WcNZ3E7X6JpAAkMWuinQIQ_PZWZPP2h1#scrollTo=El_K5tZg1AuN&line=1&uniqifier=1)."
   ],
   "metadata": {
    "id": "XrM9AMgQ3O9R"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!gdown 16Hbvud9yHTT22XZ4WLZ0g4JfWkjsikx1"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-KX19Zy53Jhl",
    "outputId": "7df9ae0e-c0fa-4edd-83ff-c46c33e4de59"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=16Hbvud9yHTT22XZ4WLZ0g4JfWkjsikx1\n",
      "To: /content/lstm_model.pth\n",
      "\r  0% 0.00/546k [00:00<?, ?B/s]\r100% 546k/546k [00:00<00:00, 13.9MB/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Параметры модели"
   ],
   "metadata": {
    "id": "xcBUr1VV0D8D"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для модели были выбраны следующие параметры:\n",
    "- входной и выходной размеры = 2, т.к. последовательность имеет размерность N x 2, т.е. ширина и долгота\n",
    "- размер скрытого слоя LSTM = 64\n",
    "- число слоёв LSTM = 2\n",
    "- размер одного батча (отрезка) при обучении модели = 128\n",
    "\n",
    "Другие характеристики:\n",
    "- В качестве предобработки данные были нормализованы в интервале от -1 до 1 для более эффективной работы функции активации в виде гиперболического тангенса, содержащейся в слое LSTM.\n",
    "- В качестве функции ошибки была выбрана среднеквадратическая ошибка (MSE).\n",
    "- Для регуляризации был выбран оптимизатор AdamW."
   ],
   "metadata": {
    "id": "-y19UyUAw1q_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sc = MinMaxScaler(feature_range=(-1, 1))  # для нормализации данных от -1 до 1\n",
    "\n",
    "# Параметры модели\n",
    "input_size = 2\n",
    "hidden_size = 64\n",
    "output_size = 2\n",
    "num_layers = 2\n",
    "batch_size = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# создаём модель\n",
    "model = LSTMModel(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "# функция потерь и оптимизатор\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "loss_fn = nn.MSELoss()"
   ],
   "metadata": {
    "id": "J00_HHpNjnSs",
    "ExecuteTime": {
     "end_time": "2025-04-01T21:11:59.323198Z",
     "start_time": "2025-04-01T21:11:59.312934Z"
    }
   },
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Загрузка и предобработка данных"
   ],
   "metadata": {
    "id": "x7Cnhx5vzQ_J"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Подгрузка выборки в виде последовательностей координат в количестве 10000 штук (routes.json)"
   ],
   "metadata": {
    "id": "bKdw4s2U0mth"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!gdown 1VYI0Mi5XTASGDon33RNMjdMlkCKynA0k"
   ],
   "metadata": {
    "id": "bu3oCSUZ0mYX",
    "ExecuteTime": {
     "end_time": "2025-03-25T18:34:55.628543Z",
     "start_time": "2025-03-25T18:34:55.572750Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"gdown\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Загрузка данных выборки из файла"
   ],
   "metadata": {
    "id": "pRLSncLRwteF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Загрузка маршрутов\n",
    "routes_path = \"training data\\\\routes.json\"\n",
    "with (open(routes_path, 'r', encoding='utf-8')) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "X, y = data['X'], data['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=42)"
   ],
   "metadata": {
    "id": "ryDnZ7uZjlX1",
    "ExecuteTime": {
     "end_time": "2025-04-01T16:44:20.067234Z",
     "start_time": "2025-04-01T16:44:13.620963Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Происходит выравнивание последовательностей в соответствии с самой длинной последовательностью и их преобразование в тензоры, далее следует нормализация в отрезок [-1; 1]. После выравнивания лишние элементы будут иметь координаты (0, 0). Затем данные загружаются в dataset и dataloader\n",
    "\n",
    "(*стоит дополнительно заняться эмбеддингом лишних элементов, чтобы модель их могла не учитывать*)"
   ],
   "metadata": {
    "id": "EPgo3PUZyE43"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X_train = [torch.tensor(sc.fit_transform(seq), dtype=torch.float32) for seq in X_train]\n",
    "y_train = [torch.tensor(sc.fit_transform(seq), dtype=torch.float32) for seq in y_train]\n",
    "X_train_pad = pad_sequence(X_train, batch_first=True)\n",
    "y_train_pad = pad_sequence(y_train, batch_first=True)\n",
    "\n",
    "X_test = [torch.tensor(sc.fit_transform(seq), dtype=torch.float32) for seq in X_test]\n",
    "y_test = [torch.tensor(sc.fit_transform(seq), dtype=torch.float32) for seq in y_test]\n",
    "X_test_pad = pad_sequence(X_test, batch_first=True)\n",
    "y_test_pad = pad_sequence(y_test, batch_first=True)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_pad, y_train_pad)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_pad, y_test_pad)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "id": "-Bk9CSmBjtkY",
    "ExecuteTime": {
     "end_time": "2025-04-01T21:12:17.585464Z",
     "start_time": "2025-04-01T21:12:08.194421Z"
    }
   },
   "outputs": [],
   "execution_count": 61
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Обучение модели"
   ],
   "metadata": {
    "id": "-nvec7ZpzWFK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "На обучение выделено 10 эпох."
   ],
   "metadata": {
    "id": "yimv0P4eykzy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "num_epochs = 10  # Количество эпох при обучении\n",
    "train_hist = []\n",
    "test_hist = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:  # выборка разделяется на части (батчи)\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        predictions = model(batch_X)\n",
    "        loss = loss_fn(predictions, batch_y)  # для каждого батча считается функция потерь\n",
    "\n",
    "        # обратное распространение ошибки\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    train_hist.append(average_loss)\n",
    "\n",
    "    # расчёты для тестовых бачтей\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_test_loss = 0.0\n",
    "\n",
    "        for batch_X_test, batch_y_test in test_loader:\n",
    "            batch_X_test, batch_y_test = batch_X_test.to(device), batch_y_test.to(device)\n",
    "            predictions_test = model(batch_X_test)\n",
    "            test_loss = loss_fn(predictions_test, batch_y_test)\n",
    "\n",
    "            total_test_loss += test_loss.item()\n",
    "\n",
    "        average_test_loss = total_test_loss / len(test_loader)\n",
    "        test_hist.append(average_test_loss)\n",
    "\n",
    "    print(\n",
    "        f'Epoch [{epoch + 1}/{num_epochs}] - Training Loss: {average_loss:.4f}, Test Loss: {average_test_loss:.4f}')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TbsdzTfjjxDq",
    "outputId": "ce7cb97e-0f4b-423b-9620-f6d0417d2b2f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch [1/10] - Training Loss: 0.0203, Test Loss: 0.0040\n",
      "Epoch [2/10] - Training Loss: 0.0040, Test Loss: 0.0037\n",
      "Epoch [3/10] - Training Loss: 0.0037, Test Loss: 0.0036\n",
      "Epoch [4/10] - Training Loss: 0.0036, Test Loss: 0.0035\n",
      "Epoch [5/10] - Training Loss: 0.0035, Test Loss: 0.0034\n",
      "Epoch [6/10] - Training Loss: 0.0035, Test Loss: 0.0034\n",
      "Epoch [7/10] - Training Loss: 0.0033, Test Loss: 0.0033\n",
      "Epoch [8/10] - Training Loss: 0.0032, Test Loss: 0.0032\n",
      "Epoch [9/10] - Training Loss: 0.0030, Test Loss: 0.0030\n",
      "Epoch [10/10] - Training Loss: 0.0029, Test Loss: 0.0028\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Параметры модели можно сохранить в отдельном файле"
   ],
   "metadata": {
    "id": "-U3JtW98y_T4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "torch.save(model, './lstm_model.pth')"
   ],
   "metadata": {
    "id": "BOQeq2XOl1Rx",
    "ExecuteTime": {
     "end_time": "2025-04-01T23:57:33.412605Z",
     "start_time": "2025-04-01T23:57:33.400036Z"
    }
   },
   "outputs": [],
   "execution_count": 73
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Пример использования"
   ],
   "metadata": {
    "id": "El_K5tZg1AuN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Загрузка файла конфигурации (config.json)."
   ],
   "metadata": {
    "id": "H4qW8lvnzCsf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!gdown 1uTIYTjQHau5R0kOhQG3lspQ-EA16SHxp"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRIJEdPpwEDS",
    "outputId": "b220dd07-e1b2-4636-a042-31e609d4a904"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1uTIYTjQHau5R0kOhQG3lspQ-EA16SHxp\n",
      "To: /content/config.json\n",
      "\r  0% 0.00/163 [00:00<?, ?B/s]\r100% 163/163 [00:00<00:00, 397kB/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Класс генератора маршрутов"
   ],
   "metadata": {
    "id": "neI78t0Y1Fs9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class RouteGenerator:\n",
    "    def __init__(self, config_path: str = \"config.json\", **kwargs):\n",
    "        \"\"\"Конструктор класса, которому в именованных аргументах передаётся\n",
    "        либо название местности, либо точные координаты местности.\n",
    "        Если было передано название, то происходит обращение к базе данных OSM,\n",
    "        где потом извлекаются точные координаты. На основе координат строится граф дорог.\n",
    "\n",
    "\n",
    "        Args:\n",
    "            config_path (str, optional): Путь к файлу конфигурации. По умолчанию стоит \"config.json\".\n",
    "\n",
    "        Raises:\n",
    "            Exception: Не были переданы ни название местности, ни его координаты.\n",
    "        \"\"\"\n",
    "\n",
    "        self.__load_config(config_path)\n",
    "        self.data = {'X': [], 'y': []}\n",
    "\n",
    "        if \"place_name\" in kwargs.keys():\n",
    "            self.__place_bbox = list(\n",
    "                ox.geocode_to_gdf(kwargs[\"place_name\"]).geometry.total_bounds\n",
    "            )\n",
    "\n",
    "        elif \"place_bbox\" in kwargs.keys():\n",
    "            self.__place_bbox = kwargs[\"place_bbox\"]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Укажите название места согласно базе данных OSM либо координаты местности.\"\n",
    "            )\n",
    "\n",
    "        self.graph = ox.graph_from_bbox(self.__place_bbox, network_type=\"drive\")  # Граф дорог местности\n",
    "\n",
    "    def __load_config(self, file_path: str) -> None:\n",
    "        \"\"\"Загрузка данных о константах через файл конфигурации.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Путь к файлу конфигурации.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(file_path, \"r\") as file:\n",
    "            config = json.load(file)\n",
    "            self.__data_amount = config[\"data_amount\"]  # Размер генерируемой выборки\n",
    "            self.__min_segment = config[\n",
    "                \"min_segment\"\n",
    "            ]  # Минимальное значение отрезка для создания отклонения\n",
    "            self.__max_segment = config[\n",
    "                \"max_segment\"\n",
    "            ]  # Максимальное значение отрезка для создания отклонения\n",
    "            self.__min_offset = config[\"min_offset\"]  # Минимальное отклонение\n",
    "            self.__max_offset = config[\"max_offset\"]  # Максимальное отклонение\n",
    "            self.__max_route_len = config[\"max_route_len\"]\n",
    "            self.__min_route_len = config[\"min_route_len\"]\n",
    "\n",
    "    def save_false_route(self, main_route: list) -> tuple:\n",
    "        \"\"\"Генерация одного искажённого маршрута на основе исходного.\n",
    "\n",
    "        Args:\n",
    "            main_route: (list): Исходный маршрут.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[nx.Graph, list]: Кортеж, внутри которого помещён изменённый граф и полученный маршрут.\n",
    "        \"\"\"\n",
    "\n",
    "        path = main_route\n",
    "        G = self.graph.copy()\n",
    "        new_nodes = [path[0]]\n",
    "\n",
    "        for i in range(len(path) - 1):\n",
    "            # Начальная и конечная точки отрезка\n",
    "            u, v = path[i], path[i + 1]\n",
    "            point1 = (G.nodes[u][\"y\"], G.nodes[u][\"x\"])\n",
    "            point2 = (G.nodes[v][\"y\"], G.nodes[v][\"x\"])\n",
    "\n",
    "            # Расстояние между узлами\n",
    "            edge_length = gd(point1, point2).meters\n",
    "            direction_bearing = ox.bearing.calculate_bearing(\n",
    "                point1[0], point1[1], point2[0], point2[1]\n",
    "            )\n",
    "\n",
    "            # Добавление точек через случайное расстояние между 20 и 60 метров\n",
    "            current_dist = 0\n",
    "            previous_node = u\n",
    "            while current_dist < edge_length:\n",
    "                # Случайное расстояние до следующей точки\n",
    "                random_dist = random.uniform(self.__min_segment, self.__max_segment)\n",
    "                current_dist += random_dist\n",
    "\n",
    "                if current_dist >= edge_length:\n",
    "                    break\n",
    "\n",
    "                # Вычисление промежуточной точки\n",
    "                new_point = gd(meters=current_dist).destination(\n",
    "                    point1, direction_bearing\n",
    "                )\n",
    "                new_lat, new_lon = new_point.latitude, new_point.longitude\n",
    "\n",
    "                # Случайное отклонение влево или вправо\n",
    "                offset_direction = direction_bearing + (\n",
    "                    90 if random.choice([True, False]) else -90\n",
    "                )\n",
    "                offset_dist = random.uniform(self.__min_offset, self.__max_offset)\n",
    "                offset_point = gd(meters=offset_dist).destination(\n",
    "                    (new_lat, new_lon), offset_direction\n",
    "                )\n",
    "                offset_lat, offset_lon = (offset_point.latitude, offset_point.longitude)\n",
    "\n",
    "                # Добавление новой вершины и её координат\n",
    "                new_node = max(G.nodes) + 1\n",
    "                G.add_node(new_node, y=offset_lat, x=offset_lon)\n",
    "                new_nodes.append(new_node)\n",
    "\n",
    "                # Добавление ребра между новой точкой и предыдущей точкой\n",
    "                G.add_edge(previous_node, new_node, length=random_dist)\n",
    "                G.add_edge(\n",
    "                    new_node, v, length=edge_length - current_dist\n",
    "                )  # Связь с основным маршрутом\n",
    "\n",
    "                previous_node = new_node  # Сместить начальную точку для следующего шага\n",
    "            new_nodes.append(path[i + 1])\n",
    "\n",
    "        false_route = [(G.nodes[n][\"x\"], G.nodes[n][\"y\"]) for n in new_nodes]\n",
    "        return G, false_route\n",
    "\n",
    "    def save_main_route(self) -> tuple:\n",
    "        \"\"\"Генерация и сохранение исходного маршрута\n",
    "\n",
    "        Args: _\n",
    "        \"\"\"\n",
    "        keys = list(self.graph.nodes.keys()).copy()\n",
    "        node_ids = []\n",
    "\n",
    "        while len(node_ids) < self.__min_route_len or len(node_ids) > self.__max_route_len:\n",
    "            try:\n",
    "                start = random.choice(keys)\n",
    "                keys.remove(start)\n",
    "                end = random.choice(keys)\n",
    "                # Поиск кратчайшего пути\n",
    "                node_ids = nx.astar_path(self.graph, start, end, weight=\"length\")\n",
    "            except nx.NetworkXNoPath:\n",
    "                pass\n",
    "        main_route = [(self.graph.nodes[n][\"x\"], self.graph.nodes[n][\"y\"])\n",
    "                      for n in node_ids]\n",
    "        return node_ids, main_route\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_cumulative_distances(route: \"np.ndarray\"):\n",
    "        distances = [0]  # Начинаем с 0 элемента\n",
    "        for i in range(1, len(route)):\n",
    "            lon1, lat1 = route[i - 1]\n",
    "            lon2, lat2 = route[i]\n",
    "            distance = haversine((lon1, lat1), (lon2, lat2), unit=Unit.METERS)\n",
    "            distances.append(distances[-1] + distance)\n",
    "        return np.array(distances)\n",
    "\n",
    "    # Функция для интерполяции маршрута\n",
    "\n",
    "    def make_equal(self, route: list, num_points: int) -> list:\n",
    "        route = np.array(route)\n",
    "        # Вычисляем кумулятивное расстояние\n",
    "        distances = self.calculate_cumulative_distances(route)\n",
    "\n",
    "        # Создаем интерполяционные функции для широты и долготы\n",
    "        interpolation_func_lon = interp1d(distances, route[:, 0], kind='linear')\n",
    "        interpolation_func_lat = interp1d(distances, route[:, 1], kind='linear')\n",
    "\n",
    "        new_distances = np.linspace(0, distances[-1], num_points)\n",
    "\n",
    "        new_lon = interpolation_func_lon(new_distances)\n",
    "        new_lat = interpolation_func_lat(new_distances)\n",
    "        new_route = list(np.column_stack((new_lon, new_lat)))\n",
    "        new_route = [tuple(point) for point in new_route]\n",
    "        return new_route\n",
    "\n",
    "    def save_data(self) -> None:\n",
    "        for i in range(self.__data_amount):\n",
    "            route_ids, main_route = self.save_main_route()\n",
    "            _, false_route = self.save_false_route(route_ids)\n",
    "\n",
    "            main_route = self.make_equal(main_route, len(false_route))\n",
    "            self.data['y'].append(main_route)\n",
    "            self.data['X'].append(false_route)\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Сделано {i + 1}/{self.__data_amount} маршрутов\")\n"
   ],
   "metadata": {
    "id": "6h9wt_KDgiAI",
    "ExecuteTime": {
     "end_time": "2025-04-01T20:40:59.628499Z",
     "start_time": "2025-04-01T20:40:59.602569Z"
    }
   },
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Функции для запуска примера"
   ],
   "metadata": {
    "id": "u73_9yRs1KUE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "В качестве примера будут созданы 3 html-файла в папке example:\n",
    "- *input* - входной маршрут\n",
    "- *target* - целевой маршрут\n",
    "- *predict* - предсказанный моделью маршрут"
   ],
   "metadata": {
    "id": "SOrS10Ci4b3M"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def save_route(points: list, save_folder: str, name: str) -> None:\n",
    "    # Создаем карту, центрированную на первой точке\n",
    "    points = [(point[1], point[0]) for point in points]\n",
    "    plot = folium.Map(location=points[0], zoom_start=15)\n",
    "\n",
    "    # Соединяем точки линией (маршрут)\n",
    "    folium.PolyLine(points, color=\"red\", weight=2, opacity=1).add_to(plot)\n",
    "\n",
    "    # Сохраняем карту в HTML-файл и открываем его\n",
    "    plot.save(f\"{save_folder}/{name}.html\")\n",
    "\n",
    "\n",
    "def lstm_test(save_folder: str) -> None:\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    # test_model = torch.load('lstm_model.pth', weights_only=False)\n",
    "    test_model = CNN1d()\n",
    "    test_model.eval()\n",
    "\n",
    "    sc = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "    place_bbox = [39.0296, 51.7806, 39.3414, 51.5301]\n",
    "\n",
    "    generator = RouteGenerator(place_bbox=place_bbox, config_path=\"configs\\\\config.json\")\n",
    "    G, result = generator.graph, generator.save_main_route()\n",
    "    main_ids, main_coords = result\n",
    "    G_false, false_coords = generator.save_false_route(main_ids)\n",
    "\n",
    "    main_coords = generator.make_equal(main_coords, len(false_coords))\n",
    "\n",
    "    save_route(main_coords, save_folder, \"target\")\n",
    "    save_route(false_coords, save_folder, \"input\")\n",
    "\n",
    "    false_coords = torch.tensor(sc.fit_transform(false_coords), dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        predict = test_model(false_coords)\n",
    "    predict = sc.inverse_transform(predict.detach().numpy())\n",
    "    print(predict)\n",
    "    save_route(predict, save_folder, \"predict\")\n",
    "    print(f\"MSE: {mean_squared_error(predict, main_coords)} \\t MAE: {mean_absolute_error(predict, main_coords)}\")\n",
    "\n"
   ],
   "metadata": {
    "id": "1KgyHYtKgVDI",
    "ExecuteTime": {
     "end_time": "2025-04-01T20:42:26.699063Z",
     "start_time": "2025-04-01T20:42:26.690794Z"
    }
   },
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "source": [
    "Запустим пример и сохраним файлы в папку example."
   ],
   "metadata": {
    "id": "W_tm9JsC1dLa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "lstm_test(\"example\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "IU7xLgQtg0c4",
    "outputId": "1187f24b-ddb7-454a-9acc-59ff4fa1d227",
    "ExecuteTime": {
     "end_time": "2025-04-01T20:42:38.382022Z",
     "start_time": "2025-04-01T20:42:29.117895Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[56]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mlstm_test\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mexample\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[55]\u001B[39m\u001B[32m, line 36\u001B[39m, in \u001B[36mlstm_test\u001B[39m\u001B[34m(save_folder)\u001B[39m\n\u001B[32m     34\u001B[39m false_coords = torch.tensor(sc.fit_transform(false_coords), dtype=torch.float32)\n\u001B[32m     35\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m     predict = \u001B[43mtest_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfalse_coords\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     37\u001B[39m predict = sc.inverse_transform(predict.detach().numpy())\n\u001B[32m     38\u001B[39m \u001B[38;5;28mprint\u001B[39m(predict)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[47]\u001B[39m\u001B[32m, line 40\u001B[39m, in \u001B[36mCNN1d.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     39\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m---> \u001B[39m\u001B[32m40\u001B[39m     x = \u001B[43mx\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpermute\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     42\u001B[39m     \u001B[38;5;66;03m# Encoder\u001B[39;00m\n\u001B[32m     43\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.encoder(x)\n",
      "\u001B[31mRuntimeError\u001B[39m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Свёрточные нейронные сети"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T16:45:56.100076Z",
     "start_time": "2025-04-01T16:45:56.096805Z"
    }
   },
   "cell_type": "code",
   "source": "import torch.nn.functional as F",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:24:08.374352Z",
     "start_time": "2025-04-01T21:24:08.366346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CNN1d(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=256, kernel_size=3, num_cnn_layers=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Расширяющаяся часть (encoder)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(input_size, hidden_size // 4, kernel_size, padding='same'),\n",
    "            nn.BatchNorm1d(hidden_size // 4),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Conv1d(hidden_size // 4, hidden_size // 2, kernel_size, padding='same'),\n",
    "            nn.BatchNorm1d(hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Середина (с остаточными связями)\n",
    "        self.mid_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(hidden_size // 2, hidden_size // 2, kernel_size, padding='same'),\n",
    "                nn.BatchNorm1d(hidden_size // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ) for _ in range(num_cnn_layers)\n",
    "        ])\n",
    "\n",
    "        # Сужающаяся часть (decoder)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv1d(hidden_size // 2, hidden_size // 4, kernel_size, padding='same'),\n",
    "            nn.BatchNorm1d(hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Conv1d(hidden_size // 4, input_size, kernel_size, padding='same'),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Middle layers with residual connections\n",
    "        for layer in self.mid_layers:\n",
    "            x = x + layer(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x.permute(0, 2, 1)"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:24:10.216662Z",
     "start_time": "2025-04-01T21:24:10.206464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = CNN1d()\n",
    "\n",
    "# функция потерь и оптимизатор\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.00005)\n",
    "\n",
    "loss_fn = nn.HuberLoss()"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T23:38:02.868590Z",
     "start_time": "2025-04-01T21:24:12.602792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 100  # Количество эпох при обучении\n",
    "train_hist = []\n",
    "test_hist = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:  # выборка разделяется на части (батчи)\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        predictions = model(batch_X)\n",
    "        loss = loss_fn(predictions, batch_y)  # для каждого батча считается функция потерь\n",
    "\n",
    "        # обратное распространение ошибки\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    train_hist.append(average_loss)\n",
    "\n",
    "    # расчёты для тестовых бачтей\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_test_loss = 0.0\n",
    "\n",
    "        for batch_X_test, batch_y_test in test_loader:\n",
    "            batch_X_test, batch_y_test = batch_X_test.to(device), batch_y_test.to(device)\n",
    "            predictions_test = model(batch_X_test)\n",
    "            test_loss = loss_fn(predictions_test, batch_y_test)\n",
    "\n",
    "            total_test_loss += test_loss.item()\n",
    "\n",
    "        average_test_loss = total_test_loss / len(test_loader)\n",
    "        test_hist.append(average_test_loss)\n",
    "\n",
    "    print(\n",
    "        f'Epoch [{epoch + 1}/{num_epochs}] - Training Loss: {average_loss:.4f}, Test Loss: {average_test_loss:.4f}')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] - Training Loss: 0.0197, Test Loss: 0.0050\n",
      "Epoch [2/100] - Training Loss: 0.0117, Test Loss: 0.0043\n",
      "Epoch [3/100] - Training Loss: 0.0093, Test Loss: 0.0042\n",
      "Epoch [4/100] - Training Loss: 0.0078, Test Loss: 0.0043\n",
      "Epoch [5/100] - Training Loss: 0.0068, Test Loss: 0.0046\n",
      "Epoch [6/100] - Training Loss: 0.0059, Test Loss: 0.0045\n",
      "Epoch [7/100] - Training Loss: 0.0053, Test Loss: 0.0041\n",
      "Epoch [8/100] - Training Loss: 0.0049, Test Loss: 0.0047\n",
      "Epoch [9/100] - Training Loss: 0.0045, Test Loss: 0.0043\n",
      "Epoch [10/100] - Training Loss: 0.0042, Test Loss: 0.0040\n",
      "Epoch [11/100] - Training Loss: 0.0039, Test Loss: 0.0037\n",
      "Epoch [12/100] - Training Loss: 0.0037, Test Loss: 0.0036\n",
      "Epoch [13/100] - Training Loss: 0.0036, Test Loss: 0.0037\n",
      "Epoch [14/100] - Training Loss: 0.0035, Test Loss: 0.0036\n",
      "Epoch [15/100] - Training Loss: 0.0033, Test Loss: 0.0034\n",
      "Epoch [16/100] - Training Loss: 0.0032, Test Loss: 0.0037\n",
      "Epoch [17/100] - Training Loss: 0.0031, Test Loss: 0.0035\n",
      "Epoch [18/100] - Training Loss: 0.0031, Test Loss: 0.0035\n",
      "Epoch [19/100] - Training Loss: 0.0030, Test Loss: 0.0031\n",
      "Epoch [20/100] - Training Loss: 0.0029, Test Loss: 0.0032\n",
      "Epoch [21/100] - Training Loss: 0.0029, Test Loss: 0.0033\n",
      "Epoch [22/100] - Training Loss: 0.0028, Test Loss: 0.0034\n",
      "Epoch [23/100] - Training Loss: 0.0028, Test Loss: 0.0035\n",
      "Epoch [24/100] - Training Loss: 0.0027, Test Loss: 0.0030\n",
      "Epoch [25/100] - Training Loss: 0.0027, Test Loss: 0.0037\n",
      "Epoch [26/100] - Training Loss: 0.0026, Test Loss: 0.0037\n",
      "Epoch [27/100] - Training Loss: 0.0026, Test Loss: 0.0035\n",
      "Epoch [28/100] - Training Loss: 0.0026, Test Loss: 0.0032\n",
      "Epoch [29/100] - Training Loss: 0.0025, Test Loss: 0.0032\n",
      "Epoch [30/100] - Training Loss: 0.0025, Test Loss: 0.0034\n",
      "Epoch [31/100] - Training Loss: 0.0025, Test Loss: 0.0033\n",
      "Epoch [32/100] - Training Loss: 0.0024, Test Loss: 0.0034\n",
      "Epoch [33/100] - Training Loss: 0.0024, Test Loss: 0.0033\n",
      "Epoch [34/100] - Training Loss: 0.0024, Test Loss: 0.0034\n",
      "Epoch [35/100] - Training Loss: 0.0023, Test Loss: 0.0034\n",
      "Epoch [36/100] - Training Loss: 0.0023, Test Loss: 0.0034\n",
      "Epoch [37/100] - Training Loss: 0.0023, Test Loss: 0.0032\n",
      "Epoch [38/100] - Training Loss: 0.0023, Test Loss: 0.0035\n",
      "Epoch [39/100] - Training Loss: 0.0023, Test Loss: 0.0033\n",
      "Epoch [40/100] - Training Loss: 0.0023, Test Loss: 0.0034\n",
      "Epoch [41/100] - Training Loss: 0.0022, Test Loss: 0.0033\n",
      "Epoch [42/100] - Training Loss: 0.0022, Test Loss: 0.0030\n",
      "Epoch [43/100] - Training Loss: 0.0022, Test Loss: 0.0033\n",
      "Epoch [44/100] - Training Loss: 0.0022, Test Loss: 0.0034\n",
      "Epoch [45/100] - Training Loss: 0.0022, Test Loss: 0.0033\n",
      "Epoch [46/100] - Training Loss: 0.0021, Test Loss: 0.0033\n",
      "Epoch [47/100] - Training Loss: 0.0021, Test Loss: 0.0035\n",
      "Epoch [48/100] - Training Loss: 0.0021, Test Loss: 0.0034\n",
      "Epoch [49/100] - Training Loss: 0.0021, Test Loss: 0.0033\n",
      "Epoch [50/100] - Training Loss: 0.0021, Test Loss: 0.0035\n",
      "Epoch [51/100] - Training Loss: 0.0021, Test Loss: 0.0034\n",
      "Epoch [52/100] - Training Loss: 0.0021, Test Loss: 0.0035\n",
      "Epoch [53/100] - Training Loss: 0.0021, Test Loss: 0.0035\n",
      "Epoch [54/100] - Training Loss: 0.0021, Test Loss: 0.0036\n",
      "Epoch [55/100] - Training Loss: 0.0020, Test Loss: 0.0035\n",
      "Epoch [56/100] - Training Loss: 0.0020, Test Loss: 0.0036\n",
      "Epoch [57/100] - Training Loss: 0.0020, Test Loss: 0.0037\n",
      "Epoch [58/100] - Training Loss: 0.0020, Test Loss: 0.0036\n",
      "Epoch [59/100] - Training Loss: 0.0020, Test Loss: 0.0037\n",
      "Epoch [60/100] - Training Loss: 0.0020, Test Loss: 0.0036\n",
      "Epoch [61/100] - Training Loss: 0.0020, Test Loss: 0.0036\n",
      "Epoch [62/100] - Training Loss: 0.0020, Test Loss: 0.0037\n",
      "Epoch [63/100] - Training Loss: 0.0020, Test Loss: 0.0037\n",
      "Epoch [64/100] - Training Loss: 0.0020, Test Loss: 0.0037\n",
      "Epoch [65/100] - Training Loss: 0.0020, Test Loss: 0.0039\n",
      "Epoch [66/100] - Training Loss: 0.0020, Test Loss: 0.0037\n",
      "Epoch [67/100] - Training Loss: 0.0020, Test Loss: 0.0039\n",
      "Epoch [68/100] - Training Loss: 0.0019, Test Loss: 0.0039\n",
      "Epoch [69/100] - Training Loss: 0.0019, Test Loss: 0.0038\n",
      "Epoch [70/100] - Training Loss: 0.0019, Test Loss: 0.0040\n",
      "Epoch [71/100] - Training Loss: 0.0019, Test Loss: 0.0040\n",
      "Epoch [72/100] - Training Loss: 0.0019, Test Loss: 0.0041\n",
      "Epoch [73/100] - Training Loss: 0.0019, Test Loss: 0.0039\n",
      "Epoch [74/100] - Training Loss: 0.0019, Test Loss: 0.0042\n",
      "Epoch [75/100] - Training Loss: 0.0019, Test Loss: 0.0040\n",
      "Epoch [76/100] - Training Loss: 0.0019, Test Loss: 0.0041\n",
      "Epoch [77/100] - Training Loss: 0.0019, Test Loss: 0.0040\n",
      "Epoch [78/100] - Training Loss: 0.0019, Test Loss: 0.0038\n",
      "Epoch [79/100] - Training Loss: 0.0019, Test Loss: 0.0039\n",
      "Epoch [80/100] - Training Loss: 0.0019, Test Loss: 0.0041\n",
      "Epoch [81/100] - Training Loss: 0.0019, Test Loss: 0.0041\n",
      "Epoch [82/100] - Training Loss: 0.0019, Test Loss: 0.0038\n",
      "Epoch [83/100] - Training Loss: 0.0019, Test Loss: 0.0042\n",
      "Epoch [84/100] - Training Loss: 0.0019, Test Loss: 0.0042\n",
      "Epoch [85/100] - Training Loss: 0.0019, Test Loss: 0.0044\n",
      "Epoch [86/100] - Training Loss: 0.0019, Test Loss: 0.0041\n",
      "Epoch [87/100] - Training Loss: 0.0019, Test Loss: 0.0045\n",
      "Epoch [88/100] - Training Loss: 0.0019, Test Loss: 0.0044\n",
      "Epoch [89/100] - Training Loss: 0.0018, Test Loss: 0.0043\n",
      "Epoch [90/100] - Training Loss: 0.0019, Test Loss: 0.0042\n",
      "Epoch [91/100] - Training Loss: 0.0019, Test Loss: 0.0041\n",
      "Epoch [92/100] - Training Loss: 0.0018, Test Loss: 0.0042\n",
      "Epoch [93/100] - Training Loss: 0.0018, Test Loss: 0.0040\n",
      "Epoch [94/100] - Training Loss: 0.0018, Test Loss: 0.0042\n",
      "Epoch [95/100] - Training Loss: 0.0018, Test Loss: 0.0042\n",
      "Epoch [96/100] - Training Loss: 0.0018, Test Loss: 0.0044\n",
      "Epoch [97/100] - Training Loss: 0.0018, Test Loss: 0.0043\n",
      "Epoch [98/100] - Training Loss: 0.0018, Test Loss: 0.0043\n",
      "Epoch [99/100] - Training Loss: 0.0018, Test Loss: 0.0043\n",
      "Epoch [100/100] - Training Loss: 0.0018, Test Loss: 0.0043\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T23:56:11.813351Z",
     "start_time": "2025-04-01T23:56:11.799031Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model, 'models/CNNs/cnn1d_1.pth')",
   "outputs": [],
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Графовые нейронные сети"
   ],
   "metadata": {
    "id": "V-AHe4vg_inD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ансамбли моделей"
   ],
   "metadata": {
    "id": "0mCzSwT0_lNF"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T22:09:17.494461Z",
     "start_time": "2025-03-26T22:09:17.490922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import geopandas as gpd"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T22:28:35.968962Z",
     "start_time": "2025-03-26T22:28:35.963250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "class GNN_RNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 graph_data: \"Data\",\n",
    "                 input_size=2,  # широта и долгота\n",
    "                 hidden_size=128,\n",
    "                 gnn_out_features=64,\n",
    "                 num_gnn_layers=2,\n",
    "                 num_lstm_layers=2,\n",
    "                 num_heads=4,\n",
    "                 dropout=0.2):\n",
    "        super(GNN_RNN, self).__init__()\n",
    "\n",
    "        self.graph_data = graph_data\n",
    "        # GNN часть (используем Graph Attention Network)\n",
    "        self.gnn_layers = nn.ModuleList()\n",
    "        self.gnn_layers.append(GATConv(input_size, hidden_size, heads=num_heads, dropout=dropout))\n",
    "        for _ in range(num_gnn_layers - 1):\n",
    "            self.gnn_layers.append(GATConv(hidden_size * num_heads, hidden_size, heads=num_heads, dropout=dropout))\n",
    "\n",
    "        self.gnn_to_lstm = nn.Linear(hidden_size * num_heads, gnn_out_features)\n",
    "\n",
    "        # RNN часть (используем LSTM)\n",
    "        self.lstm = nn.LSTM(input_size=gnn_out_features + input_size,  # объединяем координаты и признаки узлов\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_lstm_layers,\n",
    "                            dropout=dropout if num_lstm_layers > 1 else 0,\n",
    "                            bidirectional=False,\n",
    "                            batch_first=True)\n",
    "\n",
    "        # Выходной слой\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)  # предсказываем вероятность для каждого узла\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gnn_out_features = gnn_out_features\n",
    "\n",
    "    def forward(self, route_sequence, node_candidates):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            route_sequence: [batch_size, seq_len, input_size] последовательность координат маршрута\n",
    "            node_candidates: [batch_size, seq_len, max_candidates] кандидаты узлов для каждой точки маршрута\n",
    "        Returns:\n",
    "            [batch_size, seq_len, max_candidates] вероятности для каждого кандидата\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = route_sequence.size()[:2]\n",
    "        max_candidates = node_candidates.size()[2]\n",
    "\n",
    "        # 1. Обрабатываем граф с помощью GNN\n",
    "        x, edge_index = self.graph_data.x, self.graph_data.edge_index\n",
    "        for layer in self.gnn_layers:\n",
    "            x = layer(x, edge_index)\n",
    "            x = F.leaky_relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.gnn_to_lstm(x)  # [num_nodes, gnn_out_features]\n",
    "\n",
    "        # 2. Для каждого шага маршрута собираем признаки кандидатов\n",
    "        # node_candidates: [batch_size, seq_len, max_candidates]\n",
    "        # x: [num_nodes, gnn_out_features] -> расширяем для всех кандидатов\n",
    "        candidate_features = x[node_candidates.view(-1)].view(batch_size, seq_len, max_candidates,\n",
    "                                                              self.gnn_out_features)\n",
    "\n",
    "        # 3. Добавляем координаты маршрута к признакам кандидатов\n",
    "        route_expanded = route_sequence.unsqueeze(2).expand(-1, -1, max_candidates, -1)\n",
    "        lstm_input = torch.cat([candidate_features, route_expanded], dim=-1)\n",
    "\n",
    "        # 4. Обрабатываем последовательность с помощью LSTM\n",
    "        lstm_out, _ = self.lstm(lstm_input.view(-1, seq_len, self.gnn_out_features + 2))\n",
    "        lstm_out = lstm_out.contiguous().view(batch_size, seq_len, max_candidates, self.hidden_size)\n",
    "\n",
    "        # 5. Предсказываем вероятности для каждого кандидата\n",
    "        logits = self.output_layer(lstm_out).squeeze(-1)  # [batch_size, seq_len, max_candidates]\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def predict(self, graph_data, route_sequence, node_candidates, k=1):\n",
    "        \"\"\"Предсказание топ-k наиболее вероятных узлов для каждого шага маршрута\"\"\"\n",
    "        probs = self.forward(graph_data, route_sequence, node_candidates)\n",
    "        topk_probs, topk_indices = torch.topk(probs, k=k, dim=2)\n",
    "        return topk_indices, topk_probs"
   ],
   "outputs": [],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T22:27:29.319217Z",
     "start_time": "2025-03-26T22:27:27.989218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Загрузка графа дорог Воронежа\n",
    "datapath = \"training data\"\n",
    "\n",
    "# Атрибуты вершины: координаты, id, кол-вол улиц\n",
    "nodes = gpd.read_file(os.path.join(datapath, \"nodes.csv\"), encoding=\"utf8\")\n",
    "nodes = nodes.iloc[:, :4]\n",
    "nodes = nodes.astype('float32')\n",
    "\n",
    "edges = gpd.read_file(os.path.join(datapath, \"edges.csv\"), encoding=\"utf8\")\n",
    "edge_index = edges[['u', 'v']]\n",
    "edge_index = edge_index.astype('int64')"
   ],
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T22:27:29.781396Z",
     "start_time": "2025-03-26T22:27:29.776511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nodes_t = torch.tensor(nodes.values)\n",
    "edge_index_t = torch.tensor(edge_index.values)\n",
    "graph = Data(x=nodes_t, edge_index=edge_index_t)"
   ],
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T22:29:31.871301Z",
     "start_time": "2025-03-26T22:29:31.866097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Node features shape:\", graph.x.shape)  # Должно быть [N, 2]\n",
    "print(\"Edge index shape:\", graph.edge_index.shape)  # Должно быть [2, E]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features shape: torch.Size([7352, 4])\n",
      "Edge index shape: torch.Size([19200, 2])\n"
     ]
    }
   ],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T22:29:07.330501Z",
     "start_time": "2025-03-26T22:29:07.322256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Параметры модели\n",
    "out_channels = 64\n",
    "in_channels = 4\n",
    "output_size = 2\n",
    "batch_size = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# создаём модель\n",
    "model = GNN_RNN(graph).to(device)\n",
    "\n",
    "# оптимизатор\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# в качестве функции потерь - СТС, так как последовательности разнородной длины\n",
    "loss_fn = nn.CTCLoss()"
   ],
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T22:28:39.776821Z",
     "start_time": "2025-03-26T22:28:39.740978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = data['X'], data['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=42)"
   ],
   "outputs": [],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T22:28:41.803472Z",
     "start_time": "2025-03-26T22:28:40.511285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = [torch.tensor(seq, dtype=torch.float32) for seq in X_train]\n",
    "y_train = [torch.tensor(seq, dtype=torch.float32) for seq in y_train]\n",
    "X_train_pad = pad_sequence(X_train, batch_first=True)\n",
    "y_train_pad = pad_sequence(y_train, batch_first=True)\n",
    "\n",
    "X_test = [torch.tensor(seq, dtype=torch.float32) for seq in X_test]\n",
    "y_test = [torch.tensor(seq, dtype=torch.float32) for seq in y_test]\n",
    "X_test_pad = pad_sequence(X_test, batch_first=True)\n",
    "y_test_pad = pad_sequence(y_test, batch_first=True)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_pad, y_train_pad)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_pad, y_test_pad)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T22:29:09.515147Z",
     "start_time": "2025-03-26T22:29:09.293773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 50  # Количество эпох при обучении\n",
    "train_hist = []\n",
    "test_hist = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:  # выборка разделяется на части (батчи)\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        predictions = model(batch_X)\n",
    "        loss = loss_fn(predictions, batch_y)  # для каждого батча считается функция потерь\n",
    "\n",
    "        # обратное распространение ошибки\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    train_hist.append(average_loss)\n",
    "\n",
    "    # расчёты для тестовых бачтей\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_test_loss = 0.0\n",
    "\n",
    "        for batch_X_test, batch_y_test in test_loader:\n",
    "            batch_X_test, batch_y_test = batch_X_test.to(device), batch_y_test.to(device)\n",
    "            predictions_test = model(batch_X_test)\n",
    "            test_loss = loss_fn(predictions_test, batch_y_test)\n",
    "\n",
    "            total_test_loss += test_loss.item()\n",
    "\n",
    "        average_test_loss = total_test_loss / len(test_loader)\n",
    "        test_hist.append(average_test_loss)\n",
    "\n",
    "    print(\n",
    "        f'Epoch [{epoch + 1}/{num_epochs}] - Training Loss: {average_loss:.4f}, Test Loss: {average_test_loss:.4f}')\n"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 19200 but got size 2 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[127]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m batch_X, batch_y \u001B[38;5;129;01min\u001B[39;00m train_loader:  \u001B[38;5;66;03m# выборка разделяется на части (батчи)\u001B[39;00m\n\u001B[32m      8\u001B[39m     batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m     predictions = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_X\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     10\u001B[39m     loss = loss_fn(predictions, batch_y)  \u001B[38;5;66;03m# для каждого батча считается функция потерь\u001B[39;00m\n\u001B[32m     12\u001B[39m     \u001B[38;5;66;03m# обратное распространение ошибки\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[120]\u001B[39m\u001B[32m, line 36\u001B[39m, in \u001B[36mGATxRNN.forward\u001B[39m\u001B[34m(self, route_coords)\u001B[39m\n\u001B[32m     34\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, route_coords):\n\u001B[32m     35\u001B[39m     route_emb = \u001B[38;5;28mself\u001B[39m.coord_encoder(route_coords)  \u001B[38;5;66;03m# [seq_len, hidden_dim]\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m     node_emb = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgraph_data\u001B[49m\u001B[43m.\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgraph_data\u001B[49m\u001B[43m.\u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# [num_nodes, hidden_dim]\u001B[39;00m\n\u001B[32m     38\u001B[39m     \u001B[38;5;66;03m# 3. Сопоставляем точки маршрута с узлами графа через внимание\u001B[39;00m\n\u001B[32m     39\u001B[39m     corrected_emb, _ = \u001B[38;5;28mself\u001B[39m.attention(\n\u001B[32m     40\u001B[39m         route_emb, node_emb, node_emb\n\u001B[32m     41\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:349\u001B[39m, in \u001B[36mGATConv.forward\u001B[39m\u001B[34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001B[39m\n\u001B[32m    346\u001B[39m     num_nodes = \u001B[38;5;28mmin\u001B[39m(size) \u001B[38;5;28;01mif\u001B[39;00m size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m num_nodes\n\u001B[32m    347\u001B[39m     edge_index, edge_attr = remove_self_loops(\n\u001B[32m    348\u001B[39m         edge_index, edge_attr)\n\u001B[32m--> \u001B[39m\u001B[32m349\u001B[39m     edge_index, edge_attr = \u001B[43madd_self_loops\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    350\u001B[39m \u001B[43m        \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_attr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfill_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    351\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_nodes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_nodes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    352\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(edge_index, SparseTensor):\n\u001B[32m    353\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.edge_dim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch_geometric\\utils\\loop.py:472\u001B[39m, in \u001B[36madd_self_loops\u001B[39m\u001B[34m(edge_index, edge_attr, fill_value, num_nodes)\u001B[39m\n\u001B[32m    469\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    470\u001B[39m     loop_index = torch.arange(\u001B[32m0\u001B[39m, N, device=device).view(\u001B[32m1\u001B[39m, -\u001B[32m1\u001B[39m).repeat(\u001B[32m2\u001B[39m, \u001B[32m1\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m472\u001B[39m full_edge_index = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloop_index\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    474\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_sparse:\n\u001B[32m    475\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m edge_attr \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[31mRuntimeError\u001B[39m: Sizes of tensors must match except in dimension 1. Expected size 19200 but got size 2 for tensor number 1 in the list."
     ]
    }
   ],
   "execution_count": 127
  }
 ]
}
