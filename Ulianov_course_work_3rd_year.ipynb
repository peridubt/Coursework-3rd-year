{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Импорт библиотек"
      ],
      "metadata": {
        "id": "8ApPJ6x5wopM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import HuberLoss\n",
        "!pip install osmnx"
      ],
      "metadata": {
        "id": "_mdpqm6e0VuK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a2e096b-4e39-42d4-ff1c-898cf8106fc2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting osmnx\n",
            "  Downloading osmnx-2.0.2-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: geopandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from osmnx) (1.0.1)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.11/dist-packages (from osmnx) (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from osmnx) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.11/dist-packages (from osmnx) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.27 in /usr/local/lib/python3.11/dist-packages (from osmnx) (2.32.3)\n",
            "Requirement already satisfied: shapely>=2.0 in /usr/local/lib/python3.11/dist-packages (from osmnx) (2.1.0)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from geopandas>=1.0->osmnx) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from geopandas>=1.0->osmnx) (24.2)\n",
            "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from geopandas>=1.0->osmnx) (3.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->osmnx) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->osmnx) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->osmnx) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27->osmnx) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27->osmnx) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27->osmnx) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27->osmnx) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4->osmnx) (1.17.0)\n",
            "Downloading osmnx-2.0.2-py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.9/99.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: osmnx\n",
            "Successfully installed osmnx-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install haversine"
      ],
      "metadata": {
        "id": "EjCzkPrX0XzR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "369886dd-feda-4fc1-a5c1-796d25cacff1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting haversine\n",
            "  Downloading haversine-2.9.0-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading haversine-2.9.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Installing collected packages: haversine\n",
            "Successfully installed haversine-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_97UifVTjeQl",
        "ExecuteTime": {
          "end_time": "2025-04-01T16:43:59.032569Z",
          "start_time": "2025-04-01T16:43:34.553877Z"
        }
      },
      "source": [
        "import torch  # pytorch\n",
        "import json  # чтение json файла\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim  # регуляризация модели\n",
        "from torch.nn.utils.rnn import pad_sequence  # выраванивание последовательностей\n",
        "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
        "from sklearn.model_selection import train_test_split  # разделение выборки на test/train\n",
        "from sklearn.preprocessing import MinMaxScaler  # нормализация данных\n",
        "from torch.masked import masked_tensor\n",
        "\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import osmnx as ox  # библиотека для работы с OSM\n",
        "import networkx as nx  # для работы с графами местностей\n",
        "from geopy.distance import geodesic as gd\n",
        "\n",
        "from haversine import haversine, Unit  # вычисление расстояний между точками кординат\n",
        "from scipy.interpolate import interp1d  # интерполяция маршрутов\n",
        "\n",
        "import os\n",
        "import folium  # для построения html-запросов через leafnet и вывода маршрутов\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error  # метрики для модели"
      ],
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка и предобработка данных"
      ],
      "metadata": {
        "id": "x7Cnhx5vzQ_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подгрузка выборки в виде последовательностей координат в количестве 10000 штук (routes.json)"
      ],
      "metadata": {
        "id": "bKdw4s2U0mth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1VYI0Mi5XTASGDon33RNMjdMlkCKynA0k"
      ],
      "metadata": {
        "id": "bu3oCSUZ0mYX",
        "ExecuteTime": {
          "end_time": "2025-03-25T18:34:55.628543Z",
          "start_time": "2025-03-25T18:34:55.572750Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83fe8858-54e4-456b-9316-7b22f482a438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1VYI0Mi5XTASGDon33RNMjdMlkCKynA0k\n",
            "From (redirected): https://drive.google.com/uc?id=1VYI0Mi5XTASGDon33RNMjdMlkCKynA0k&confirm=t&uuid=0f282060-d26f-45e4-84e8-332d524e87e2\n",
            "To: /content/routes.json\n",
            "100% 286M/286M [00:06<00:00, 47.2MB/s]\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка данных выборки из файла"
      ],
      "metadata": {
        "id": "pRLSncLRwteF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка маршрутов\n",
        "routes_path = \"routes.json\"\n",
        "with (open(routes_path, 'r', encoding='utf-8')) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "X, y = data['X'], data['y']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    train_size=0.8,\n",
        "                                                    random_state=42)"
      ],
      "metadata": {
        "id": "ryDnZ7uZjlX1",
        "ExecuteTime": {
          "end_time": "2025-04-01T16:44:20.067234Z",
          "start_time": "2025-04-01T16:44:13.620963Z"
        }
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "Происходит выравнивание последовательностей в соответствии с самой длинной последовательностью и их преобразование в тензоры, далее следует нормализация в отрезок [-1; 1]. После выравнивания лишние элементы будут иметь координаты (0, 0). Затем данные загружаются в dataset и dataloader\n",
        "\n",
        "Размер одного батча (отрезка) при обучении моделей = 64"
      ],
      "metadata": {
        "id": "EPgo3PUZyE43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = MinMaxScaler(feature_range=(-1, 1))\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "iA5EXTlDya2s"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = [torch.tensor(sc.fit_transform(seq), dtype=torch.float32) for seq in X_train]\n",
        "y_train = [torch.tensor(sc.fit_transform(seq), dtype=torch.float32) for seq in y_train]\n",
        "X_train_pad = pad_sequence(X_train, batch_first=True)\n",
        "y_train_pad = pad_sequence(y_train, batch_first=True)\n",
        "\n",
        "X_test = [torch.tensor(sc.fit_transform(seq), dtype=torch.float32) for seq in X_test]\n",
        "y_test = [torch.tensor(sc.fit_transform(seq), dtype=torch.float32) for seq in y_test]\n",
        "X_test_pad = pad_sequence(X_test, batch_first=True)\n",
        "y_test_pad = pad_sequence(y_test, batch_first=True)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_pad, y_train_pad)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataset = TensorDataset(X_test_pad, y_test_pad)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "-Bk9CSmBjtkY",
        "ExecuteTime": {
          "end_time": "2025-04-01T21:12:17.585464Z",
          "start_time": "2025-04-01T21:12:08.194421Z"
        }
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Рекуррентные нейронные сети"
      ],
      "metadata": {
        "id": "isXu_940_Hl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Простейшая LSTM"
      ],
      "metadata": {
        "id": "J6Fy9sNXTyi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # проходим через LSTM\n",
        "        out, _ = self.lstm(x)\n",
        "        # проходим через линейный слой\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "ZwPmCw0WT8DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Загрузка обученной модели (опционально)**"
      ],
      "metadata": {
        "id": "o7el3OJYytXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно пропустить этап с обучением и сразу загрузить обученную модель (lstm_model.pth) с установившимися параметрами и перейти к шагу с [примером](https://colab.research.google.com/drive/1l7Huzgg3UWtsEOXKwKq9BCVu36yVRGG-#scrollTo=q4jUyVBLW1Ep&line=1&uniqifier=1). Подобное можно проделать и с остальными моделями."
      ],
      "metadata": {
        "id": "XrM9AMgQ3O9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Параметры модели"
      ],
      "metadata": {
        "id": "R6obs4cEzA9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение модели"
      ],
      "metadata": {
        "id": "qIHnoQyozEJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Двунаправленная LSTM"
      ],
      "metadata": {
        "id": "iqxMaMolz8-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Слои LSTM являются двунаправленными, т.е. они могут работать как с данными из прошлого, так и с предсказанными, отсюда для линейного слоя размер скрытого слоя увеличен в 2 раза. Также был добавлен слой dropout, который отвечает за отключение неиспользуемых нейронов"
      ],
      "metadata": {
        "id": "7JYK6an-wqJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BidirectionalLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(BidirectionalLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # проходим через LSTM\n",
        "        out, _ = self.lstm(x)\n",
        "        # проходим через линейный слой\n",
        "        out = self.fc(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "0d-8cn4djhzk",
        "ExecuteTime": {
          "end_time": "2025-04-01T16:44:06.545001Z",
          "start_time": "2025-04-01T16:44:06.540206Z"
        }
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Загрузка обученной модели (опционально)**"
      ],
      "metadata": {
        "id": "uOdXM18J3Ijx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 16Hbvud9yHTT22XZ4WLZ0g4JfWkjsikx1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KX19Zy53Jhl",
        "outputId": "7df9ae0e-c0fa-4edd-83ff-c46c33e4de59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16Hbvud9yHTT22XZ4WLZ0g4JfWkjsikx1\n",
            "To: /content/lstm_model.pth\n",
            "\r  0% 0.00/546k [00:00<?, ?B/s]\r100% 546k/546k [00:00<00:00, 13.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Параметры модели"
      ],
      "metadata": {
        "id": "xcBUr1VV0D8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для модели были выбраны следующие параметры:\n",
        "- входной и выходной размеры = 2, т.к. последовательность имеет размерность N x 2, т.е. ширина и долгота\n",
        "- размер скрытого слоя LSTM = 64\n",
        "- число слоёв LSTM = 2\n",
        "\n",
        "Другие характеристики:\n",
        "- В качестве предобработки данные были нормализованы в интервале от -1 до 1 для более эффективной работы функции активации в виде гиперболического тангенса, содержащейся в слое LSTM.\n",
        "- В качестве функции ошибки была выбрана среднеквадратическая ошибка (MSE).\n",
        "- Для регуляризации был выбран оптимизатор AdamW."
      ],
      "metadata": {
        "id": "-y19UyUAw1q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Параметры модели\n",
        "input_size = 2\n",
        "hidden_size = 64\n",
        "output_size = 2\n",
        "num_layers = 2\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# создаём модель\n",
        "model = LSTMModel(input_size, hidden_size, output_size, num_layers)\n",
        "\n",
        "# функция потерь и оптимизатор\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
        "\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "J00_HHpNjnSs",
        "ExecuteTime": {
          "end_time": "2025-04-01T21:11:59.323198Z",
          "start_time": "2025-04-01T21:11:59.312934Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение модели"
      ],
      "metadata": {
        "id": "-nvec7ZpzWFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "На обучение выделено 10 эпох."
      ],
      "metadata": {
        "id": "yimv0P4eykzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10  # Количество эпох при обучении\n",
        "train_hist = []\n",
        "test_hist = []\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    model.train()\n",
        "    for batch_X, batch_y in train_loader:  # выборка разделяется на части (батчи)\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        predictions = model(batch_X)\n",
        "        loss = loss_fn(predictions, batch_y)  # для каждого батча считается функция потерь\n",
        "\n",
        "        # обратное распространение ошибки\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    train_hist.append(average_loss)\n",
        "\n",
        "    # расчёты для тестовых бачтей\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_test_loss = 0.0\n",
        "\n",
        "        for batch_X_test, batch_y_test in test_loader:\n",
        "            batch_X_test, batch_y_test = batch_X_test.to(device), batch_y_test.to(device)\n",
        "            predictions_test = model(batch_X_test)\n",
        "            test_loss = loss_fn(predictions_test, batch_y_test)\n",
        "\n",
        "            total_test_loss += test_loss.item()\n",
        "\n",
        "        average_test_loss = total_test_loss / len(test_loader)\n",
        "        test_hist.append(average_test_loss)\n",
        "\n",
        "    print(\n",
        "        f'Epoch [{epoch + 1}/{num_epochs}] - Training Loss: {average_loss:.4f}, Test Loss: {average_test_loss:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbsdzTfjjxDq",
        "outputId": "ce7cb97e-0f4b-423b-9620-f6d0417d2b2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - Training Loss: 0.0203, Test Loss: 0.0040\n",
            "Epoch [2/10] - Training Loss: 0.0040, Test Loss: 0.0037\n",
            "Epoch [3/10] - Training Loss: 0.0037, Test Loss: 0.0036\n",
            "Epoch [4/10] - Training Loss: 0.0036, Test Loss: 0.0035\n",
            "Epoch [5/10] - Training Loss: 0.0035, Test Loss: 0.0034\n",
            "Epoch [6/10] - Training Loss: 0.0035, Test Loss: 0.0034\n",
            "Epoch [7/10] - Training Loss: 0.0033, Test Loss: 0.0033\n",
            "Epoch [8/10] - Training Loss: 0.0032, Test Loss: 0.0032\n",
            "Epoch [9/10] - Training Loss: 0.0030, Test Loss: 0.0030\n",
            "Epoch [10/10] - Training Loss: 0.0029, Test Loss: 0.0028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Параметры модели можно сохранить в отдельном файле"
      ],
      "metadata": {
        "id": "-U3JtW98y_T4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, './lstm_model.pth')"
      ],
      "metadata": {
        "id": "BOQeq2XOl1Rx",
        "ExecuteTime": {
          "end_time": "2025-04-01T23:57:33.412605Z",
          "start_time": "2025-04-01T23:57:33.400036Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запустим пример и сохраним файлы в папку example."
      ],
      "metadata": {
        "id": "W_tm9JsC1dLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_test(\"example\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IU7xLgQtg0c4",
        "outputId": "1187f24b-ddb7-454a-9acc-59ff4fa1d227",
        "ExecuteTime": {
          "end_time": "2025-04-01T20:42:38.382022Z",
          "start_time": "2025-04-01T20:42:29.117895Z"
        }
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mlstm_test\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexample\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mlstm_test\u001b[39m\u001b[34m(save_folder)\u001b[39m\n\u001b[32m     34\u001b[39m false_coords = torch.tensor(sc.fit_transform(false_coords), dtype=torch.float32)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     predict = \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfalse_coords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m predict = sc.inverse_transform(predict.detach().numpy())\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(predict)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mCNN1d.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     x = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# Encoder\u001b[39;00m\n\u001b[32m     43\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.encoder(x)\n",
            "\u001b[31mRuntimeError\u001b[39m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Улучшенная модель LSTM"
      ],
      "metadata": {
        "id": "v08WpXeYTm9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class EnhancedLSTM(nn.Module):\n",
        "    def __init__(self, input_size=2, hidden_size=256, output_size=2, num_layers=3):\n",
        "        super(EnhancedLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Улучшенная LSTM с:\n",
        "        # - Больше слоев\n",
        "        # - Layer normalization\n",
        "        # - Dropout между слоями\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0.3 if num_layers > 1 else 0,\n",
        "            bidirectional=False\n",
        "        )\n",
        "\n",
        "        # Layer normalization для стабилизации LSTM выходов\n",
        "        self.ln1 = nn.LayerNorm(hidden_size)\n",
        "\n",
        "        # Дополнительные полносвязные слои с residual connection\n",
        "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.ln2 = nn.LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # Инициализация весов\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for name, param in self.lstm.named_parameters():\n",
        "            if 'weight_ih' in name:\n",
        "                nn.init.xavier_uniform_(param.data)\n",
        "            elif 'weight_hh' in name:\n",
        "                nn.init.orthogonal_(param.data)\n",
        "            elif 'bias' in name:\n",
        "                param.data.fill_(0)\n",
        "                # Устанавливаем forget gate bias в 1\n",
        "                n = param.size(0)\n",
        "                param.data[(n // 4):(n // 2)].fill_(1)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.zeros_(self.fc1.bias)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # LSTM layers\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.ln1(out)\n",
        "\n",
        "        # Первый полносвязный слой с residual connection\n",
        "        residual = out\n",
        "        out = self.fc1(out)\n",
        "        out = self.ln2(out)\n",
        "        out = F.leaky_relu(out, 0.1)\n",
        "        out = self.dropout(out)\n",
        "        out = out + residual[:, :, :self.hidden_size]  # residual connection\n",
        "\n",
        "        # Final output layer\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"Инференс для одного маршрута\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            if isinstance(x, (list, np.ndarray)):\n",
        "                x = torch.FloatTensor(x).unsqueeze(0)  # add batch dim\n",
        "            return self.forward(x).squeeze(0).cpu().numpy()"
      ],
      "metadata": {
        "id": "geKs0zv0Trb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Загрузка обученной модели (опционально)**"
      ],
      "metadata": {
        "id": "2_VBXBrbzv6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Параметры модели"
      ],
      "metadata": {
        "id": "ofGNyxEezG-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение модели"
      ],
      "metadata": {
        "id": "ZLJ_XzJkzKQz"
      }
    },
    {
      "metadata": {
        "id": "RL-LqHmFNNhD"
      },
      "cell_type": "markdown",
      "source": [
        "# Свёрточные нейронные сети"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-01T16:45:56.100076Z",
          "start_time": "2025-04-01T16:45:56.096805Z"
        },
        "id": "HTpk9bIkNNhD"
      },
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Простейшая одномерная CNN"
      ],
      "metadata": {
        "id": "lKvwc8YrVjXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Параметры модели"
      ],
      "metadata": {
        "id": "Gp-KMOmLzVTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение модели"
      ],
      "metadata": {
        "id": "M-nMx_2OzXCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Улучшенная одномерная CNN"
      ],
      "metadata": {
        "id": "lAU4BuALVnlK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Параметры модели"
      ],
      "metadata": {
        "id": "kuD2EQOHzbzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение модели"
      ],
      "metadata": {
        "id": "J3W-pTMozbzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Одномерная CNN на основе encoder и decoder"
      ],
      "metadata": {
        "id": "N1hpcQBfVr9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VApmxMlcVr7d"
      }
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-01T21:24:08.374352Z",
          "start_time": "2025-04-01T21:24:08.366346Z"
        },
        "id": "Qk986kB_NNhP"
      },
      "cell_type": "code",
      "source": [
        "class CNN1d(nn.Module):\n",
        "    def __init__(self, input_size=2, hidden_size=256, kernel_size=3, num_cnn_layers=4, dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Расширяющаяся часть (encoder)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(input_size, hidden_size // 4, kernel_size, padding='same'),\n",
        "            nn.BatchNorm1d(hidden_size // 4),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            nn.Conv1d(hidden_size // 4, hidden_size // 2, kernel_size, padding='same'),\n",
        "            nn.BatchNorm1d(hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Середина (с остаточными связями)\n",
        "        self.mid_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_size // 2, hidden_size // 2, kernel_size, padding='same'),\n",
        "                nn.BatchNorm1d(hidden_size // 2),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ) for _ in range(num_cnn_layers)\n",
        "        ])\n",
        "\n",
        "        # Сужающаяся часть (decoder)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv1d(hidden_size // 2, hidden_size // 4, kernel_size, padding='same'),\n",
        "            nn.BatchNorm1d(hidden_size // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            nn.Conv1d(hidden_size // 4, input_size, kernel_size, padding='same'),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Encoder\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        # Middle layers with residual connections\n",
        "        for layer in self.mid_layers:\n",
        "            x = x + layer(x)\n",
        "\n",
        "        # Decoder\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        return x.permute(0, 2, 1)"
      ],
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Загрузка обученной модели (опционально)**"
      ],
      "metadata": {
        "id": "uOmARPqPzmud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Параметры модели"
      ],
      "metadata": {
        "id": "IZ33jXnrzfDi"
      }
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-01T21:24:10.216662Z",
          "start_time": "2025-04-01T21:24:10.206464Z"
        },
        "id": "Cpi5OiPvNNhQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "d97397a5-3cdb-4340-a945-81e7b370a612"
      },
      "cell_type": "code",
      "source": [
        "model = CNN1d()\n",
        "\n",
        "# функция потерь и оптимизатор\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.00005)\n",
        "\n",
        "loss_fn = nn.L1Loss()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c1abfb7e74dd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# функция потерь и оптимизатор\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, maximize, foreach, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mfused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfused\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         )\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_param_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;31m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mdisable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__dynamo_disable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdisable_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mdisable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallback_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlobalStateGuard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_compile_pg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_convert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorifyState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorifyScalarRestartAnalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTracingContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/exc.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcounters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/experimental/symbolic_shapes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ordered_set\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_dispatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_traceable_wrapper_subclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m from torch.utils._sympy.functions import (\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mApplication\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mCeilToInt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_sympy/functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msympify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sympy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m         assuming, Q, ask, register_handler, remove_handler, refine)\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m from .polys import (Poly, PurePoly, poly_from_expr, parallel_poly_from_expr,\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_degree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpquo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mpexquo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexquo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf_gcdex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcdex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sympy/polys/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m ]\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from .polytools import (Poly, PurePoly, poly_from_expr,\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mparallel_poly_from_expr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_degree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mLT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpquo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpexquo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexquo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf_gcdex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcdex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение модели"
      ],
      "metadata": {
        "id": "4gup0sxYzfDj"
      }
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-01T23:38:02.868590Z",
          "start_time": "2025-04-01T21:24:12.602792Z"
        },
        "id": "luv3Ki-GNNhQ"
      },
      "cell_type": "code",
      "source": [
        "num_epochs = 100  # Количество эпох при обучении\n",
        "train_hist = []\n",
        "test_hist = []\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    model.train()\n",
        "    for batch_X, batch_y in train_loader:  # выборка разделяется на части (батчи)\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        predictions = model(batch_X)\n",
        "        loss = loss_fn(predictions, batch_y)  # для каждого батча считается функция потерь\n",
        "\n",
        "        # обратное распространение ошибки\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    train_hist.append(average_loss)\n",
        "\n",
        "    # расчёты для тестовых бачтей\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_test_loss = 0.0\n",
        "\n",
        "        for batch_X_test, batch_y_test in test_loader:\n",
        "            batch_X_test, batch_y_test = batch_X_test.to(device), batch_y_test.to(device)\n",
        "            predictions_test = model(batch_X_test)\n",
        "            test_loss = loss_fn(predictions_test, batch_y_test)\n",
        "\n",
        "            total_test_loss += test_loss.item()\n",
        "\n",
        "        average_test_loss = total_test_loss / len(test_loader)\n",
        "        test_hist.append(average_test_loss)\n",
        "\n",
        "    print(\n",
        "        f'Epoch [{epoch + 1}/{num_epochs}] - Training Loss: {average_loss:.4f}, Test Loss: {average_test_loss:.4f}')\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-01T23:56:11.813351Z",
          "start_time": "2025-04-01T23:56:11.799031Z"
        },
        "id": "Z6xR82JeNNhQ"
      },
      "cell_type": "code",
      "source": [
        "torch.save(model, 'models/CNNs/cnn1d_1.pth')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ансамбли моделей"
      ],
      "metadata": {
        "id": "0mCzSwT0_lNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Стэкинг GAT и LSTM"
      ],
      "metadata": {
        "id": "DqMHhCniXJgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S_Xc1uqxj2H",
        "outputId": "7a8c9964-c140-4964-a634-d61aca859c98"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-26T22:09:17.494461Z",
          "start_time": "2025-03-26T22:09:17.490922Z"
        },
        "id": "xlrX02rRNNhR"
      },
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.data import Data, Dataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "import geopandas as gpd"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-26T22:28:35.968962Z",
          "start_time": "2025-03-26T22:28:35.963250Z"
        },
        "id": "ryi7Qn3gNNhR"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "\n",
        "class GNN_RNN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 graph_data: \"Data\",\n",
        "                 input_size=2,  # широта и долгота\n",
        "                 hidden_size=128,\n",
        "                 gnn_out_features=64,\n",
        "                 num_gnn_layers=2,\n",
        "                 num_lstm_layers=2,\n",
        "                 num_heads=4,\n",
        "                 dropout=0.2):\n",
        "        super(GNN_RNN, self).__init__()\n",
        "\n",
        "        self.graph_data = graph_data\n",
        "        # GNN часть (используем Graph Attention Network)\n",
        "        self.gnn_layers = nn.ModuleList()\n",
        "        self.gnn_layers.append(GATConv(input_size, hidden_size, heads=num_heads, dropout=dropout))\n",
        "        for _ in range(num_gnn_layers - 1):\n",
        "            self.gnn_layers.append(GATConv(hidden_size * num_heads, hidden_size, heads=num_heads, dropout=dropout))\n",
        "\n",
        "        self.gnn_to_lstm = nn.Linear(hidden_size * num_heads, gnn_out_features)\n",
        "\n",
        "        # RNN часть (используем LSTM)\n",
        "        self.lstm = nn.LSTM(input_size=gnn_out_features + input_size,  # объединяем координаты и признаки узлов\n",
        "                            hidden_size=hidden_size,\n",
        "                            num_layers=num_lstm_layers,\n",
        "                            dropout=dropout if num_lstm_layers > 1 else 0,\n",
        "                            bidirectional=False,\n",
        "                            batch_first=True)\n",
        "\n",
        "        # Выходной слой\n",
        "        self.output_layer = nn.Linear(hidden_size, 1)  # предсказываем вероятность для каждого узла\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gnn_out_features = gnn_out_features\n",
        "\n",
        "    def forward(self, route_sequence, node_candidates):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            route_sequence: [batch_size, seq_len, input_size] последовательность координат маршрута\n",
        "            node_candidates: [batch_size, seq_len, max_candidates] кандидаты узлов для каждой точки маршрута\n",
        "        Returns:\n",
        "            [batch_size, seq_len, max_candidates] вероятности для каждого кандидата\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = route_sequence.size()[:2]\n",
        "        max_candidates = node_candidates.size()[2]\n",
        "\n",
        "        # 1. Обрабатываем граф с помощью GNN\n",
        "        x, edge_index = self.graph_data.x, self.graph_data.edge_index\n",
        "        for layer in self.gnn_layers:\n",
        "            x = layer(x, edge_index)\n",
        "            x = F.leaky_relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        x = self.gnn_to_lstm(x)  # [num_nodes, gnn_out_features]\n",
        "\n",
        "        # 2. Для каждого шага маршрута собираем признаки кандидатов\n",
        "        # node_candidates: [batch_size, seq_len, max_candidates]\n",
        "        # x: [num_nodes, gnn_out_features] -> расширяем для всех кандидатов\n",
        "        candidate_features = x[node_candidates.view(-1)].view(batch_size, seq_len, max_candidates,\n",
        "                                                              self.gnn_out_features)\n",
        "\n",
        "        # 3. Добавляем координаты маршрута к признакам кандидатов\n",
        "        route_expanded = route_sequence.unsqueeze(2).expand(-1, -1, max_candidates, -1)\n",
        "        lstm_input = torch.cat([candidate_features, route_expanded], dim=-1)\n",
        "\n",
        "        # 4. Обрабатываем последовательность с помощью LSTM\n",
        "        lstm_out, _ = self.lstm(lstm_input.view(-1, seq_len, self.gnn_out_features + 2))\n",
        "        lstm_out = lstm_out.contiguous().view(batch_size, seq_len, max_candidates, self.hidden_size)\n",
        "\n",
        "        # 5. Предсказываем вероятности для каждого кандидата\n",
        "        logits = self.output_layer(lstm_out).squeeze(-1)  # [batch_size, seq_len, max_candidates]\n",
        "        probs = torch.sigmoid(logits)\n",
        "\n",
        "        return probs\n",
        "\n",
        "    def predict(self, graph_data, route_sequence, node_candidates, k=1):\n",
        "        \"\"\"Предсказание топ-k наиболее вероятных узлов для каждого шага маршрута\"\"\"\n",
        "        probs = self.forward(graph_data, route_sequence, node_candidates)\n",
        "        topk_probs, topk_indices = torch.topk(probs, k=k, dim=2)\n",
        "        return topk_indices, topk_probs"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-26T22:27:29.319217Z",
          "start_time": "2025-03-26T22:27:27.989218Z"
        },
        "id": "OvOQJpPqNNhR"
      },
      "cell_type": "code",
      "source": [
        "# Загрузка графа дорог Воронежа\n",
        "datapath = \"training data\"\n",
        "\n",
        "# Атрибуты вершины: координаты, id, кол-вол улиц\n",
        "nodes = gpd.read_file(os.path.join(datapath, \"nodes.csv\"), encoding=\"utf8\")\n",
        "nodes = nodes.iloc[:, :4]\n",
        "nodes = nodes.astype('float32')\n",
        "\n",
        "edges = gpd.read_file(os.path.join(datapath, \"edges.csv\"), encoding=\"utf8\")\n",
        "edge_index = edges[['u', 'v']]\n",
        "edge_index = edge_index.astype('int64')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-26T22:27:29.781396Z",
          "start_time": "2025-03-26T22:27:29.776511Z"
        },
        "id": "Ee2N303oNNhS"
      },
      "cell_type": "code",
      "source": [
        "nodes_t = torch.tensor(nodes.values)\n",
        "edge_index_t = torch.tensor(edge_index.values)\n",
        "graph = Data(x=nodes_t, edge_index=edge_index_t)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-26T22:29:31.871301Z",
          "start_time": "2025-03-26T22:29:31.866097Z"
        },
        "id": "baVkGb6TNNhS",
        "outputId": "7552894a-0574-4742-c5e7-05f33264a83f"
      },
      "cell_type": "code",
      "source": [
        "print(\"Node features shape:\", graph.x.shape)  # Должно быть [N, 2]\n",
        "print(\"Edge index shape:\", graph.edge_index.shape)  # Должно быть [2, E]"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node features shape: torch.Size([7352, 4])\n",
            "Edge index shape: torch.Size([19200, 2])\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-26T22:29:07.330501Z",
          "start_time": "2025-03-26T22:29:07.322256Z"
        },
        "id": "UdX2IaqANNhS"
      },
      "cell_type": "code",
      "source": [
        "# Параметры модели\n",
        "out_channels = 64\n",
        "in_channels = 4\n",
        "output_size = 2\n",
        "batch_size = 128\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# создаём модель\n",
        "model = GNN_RNN(graph).to(device)\n",
        "\n",
        "# оптимизатор\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# в качестве функции потерь - СТС, так как последовательности разнородной длины\n",
        "loss_fn = nn.CTCLoss()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-26T22:28:39.776821Z",
          "start_time": "2025-03-26T22:28:39.740978Z"
        },
        "id": "aqiCEGbtNNhS"
      },
      "cell_type": "code",
      "source": [
        "X, y = data['X'], data['y']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    train_size=0.8,\n",
        "                                                    random_state=42)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-26T22:28:41.803472Z",
          "start_time": "2025-03-26T22:28:40.511285Z"
        },
        "id": "r-vmZ62aNNhS"
      },
      "cell_type": "code",
      "source": [
        "X_train = [torch.tensor(seq, dtype=torch.float32) for seq in X_train]\n",
        "y_train = [torch.tensor(seq, dtype=torch.float32) for seq in y_train]\n",
        "X_train_pad = pad_sequence(X_train, batch_first=True)\n",
        "y_train_pad = pad_sequence(y_train, batch_first=True)\n",
        "\n",
        "X_test = [torch.tensor(seq, dtype=torch.float32) for seq in X_test]\n",
        "y_test = [torch.tensor(seq, dtype=torch.float32) for seq in y_test]\n",
        "X_test_pad = pad_sequence(X_test, batch_first=True)\n",
        "y_test_pad = pad_sequence(y_test, batch_first=True)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_pad, y_train_pad)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataset = TensorDataset(X_test_pad, y_test_pad)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-26T22:29:09.515147Z",
          "start_time": "2025-03-26T22:29:09.293773Z"
        },
        "id": "PLt1QU8PNNhT",
        "outputId": "ab868b6d-abb7-4ded-f0e7-a758dbbbf2c8"
      },
      "cell_type": "code",
      "source": [
        "num_epochs = 50  # Количество эпох при обучении\n",
        "train_hist = []\n",
        "test_hist = []\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    model.train()\n",
        "    for batch_X, batch_y in train_loader:  # выборка разделяется на части (батчи)\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        predictions = model(batch_X)\n",
        "        loss = loss_fn(predictions, batch_y)  # для каждого батча считается функция потерь\n",
        "\n",
        "        # обратное распространение ошибки\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    train_hist.append(average_loss)\n",
        "\n",
        "    # расчёты для тестовых бачтей\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_test_loss = 0.0\n",
        "\n",
        "        for batch_X_test, batch_y_test in test_loader:\n",
        "            batch_X_test, batch_y_test = batch_X_test.to(device), batch_y_test.to(device)\n",
        "            predictions_test = model(batch_X_test)\n",
        "            test_loss = loss_fn(predictions_test, batch_y_test)\n",
        "\n",
        "            total_test_loss += test_loss.item()\n",
        "\n",
        "        average_test_loss = total_test_loss / len(test_loader)\n",
        "        test_hist.append(average_test_loss)\n",
        "\n",
        "    print(\n",
        "        f'Epoch [{epoch + 1}/{num_epochs}] - Training Loss: {average_loss:.4f}, Test Loss: {average_test_loss:.4f}')\n"
      ],
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Sizes of tensors must match except in dimension 1. Expected size 19200 but got size 2 for tensor number 1 in the list.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[127]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:  \u001b[38;5;66;03m# выборка разделяется на части (батчи)\u001b[39;00m\n\u001b[32m      8\u001b[39m     batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     predictions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     loss = loss_fn(predictions, batch_y)  \u001b[38;5;66;03m# для каждого батча считается функция потерь\u001b[39;00m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# обратное распространение ошибки\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[120]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mGATxRNN.forward\u001b[39m\u001b[34m(self, route_coords)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, route_coords):\n\u001b[32m     35\u001b[39m     route_emb = \u001b[38;5;28mself\u001b[39m.coord_encoder(route_coords)  \u001b[38;5;66;03m# [seq_len, hidden_dim]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     node_emb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgraph_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgraph_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [num_nodes, hidden_dim]\u001b[39;00m\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# 3. Сопоставляем точки маршрута с узлами графа через внимание\u001b[39;00m\n\u001b[32m     39\u001b[39m     corrected_emb, _ = \u001b[38;5;28mself\u001b[39m.attention(\n\u001b[32m     40\u001b[39m         route_emb, node_emb, node_emb\n\u001b[32m     41\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:349\u001b[39m, in \u001b[36mGATConv.forward\u001b[39m\u001b[34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[39m\n\u001b[32m    346\u001b[39m     num_nodes = \u001b[38;5;28mmin\u001b[39m(size) \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m num_nodes\n\u001b[32m    347\u001b[39m     edge_index, edge_attr = remove_self_loops(\n\u001b[32m    348\u001b[39m         edge_index, edge_attr)\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m     edge_index, edge_attr = \u001b[43madd_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, SparseTensor):\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.edge_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch_geometric\\utils\\loop.py:472\u001b[39m, in \u001b[36madd_self_loops\u001b[39m\u001b[34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    470\u001b[39m     loop_index = torch.arange(\u001b[32m0\u001b[39m, N, device=device).view(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m).repeat(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m full_edge_index = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sparse:\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m edge_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[31mRuntimeError\u001b[39m: Sizes of tensors must match except in dimension 1. Expected size 19200 but got size 2 for tensor number 1 in the list."
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тестирование моделей"
      ],
      "metadata": {
        "id": "1IsHsTYAUS2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка файла конфигурации (config.json). Он передаётся классу, который описан ниже."
      ],
      "metadata": {
        "id": "H4qW8lvnzCsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1uTIYTjQHau5R0kOhQG3lspQ-EA16SHxp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRIJEdPpwEDS",
        "outputId": "b220dd07-e1b2-4636-a042-31e609d4a904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1uTIYTjQHau5R0kOhQG3lspQ-EA16SHxp\n",
            "To: /content/config.json\n",
            "\r  0% 0.00/163 [00:00<?, ?B/s]\r100% 163/163 [00:00<00:00, 397kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Класс генератора маршрутов"
      ],
      "metadata": {
        "id": "neI78t0Y1Fs9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный класс использовался для генерации 10.000 пар маршрутов для выборки. С помощью него также можно получить граф дорог, который получается с помощью заключения выбранной области в прямоугольник. Для этого выбирается верхняя левая и нижняя правая точки."
      ],
      "metadata": {
        "id": "3sKb8V3WUlkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RouteGenerator:\n",
        "    def __init__(self, config_path: str = \"config.json\", **kwargs):\n",
        "        \"\"\"Конструктор класса, которому в именованных аргументах передаётся\n",
        "        либо название местности, либо точные координаты местности.\n",
        "        Если было передано название, то происходит обращение к базе данных OSM,\n",
        "        где потом извлекаются точные координаты. На основе координат строится граф дорог.\n",
        "\n",
        "\n",
        "        Args:\n",
        "            config_path (str, optional): Путь к файлу конфигурации. По умолчанию стоит \"config.json\".\n",
        "\n",
        "        Raises:\n",
        "            Exception: Не были переданы ни название местности, ни его координаты.\n",
        "        \"\"\"\n",
        "\n",
        "        self.__load_config(config_path)\n",
        "        self.data = {'X': [], 'y': []}\n",
        "\n",
        "        if \"place_name\" in kwargs.keys():\n",
        "            self.__place_bbox = list(\n",
        "                ox.geocode_to_gdf(kwargs[\"place_name\"]).geometry.total_bounds\n",
        "            )\n",
        "\n",
        "        elif \"place_bbox\" in kwargs.keys():\n",
        "            self.__place_bbox = kwargs[\"place_bbox\"]\n",
        "        else:\n",
        "            raise Exception(\n",
        "                \"Укажите название места согласно базе данных OSM либо координаты местности.\"\n",
        "            )\n",
        "\n",
        "        self.graph = ox.graph_from_bbox(self.__place_bbox, network_type=\"drive\")  # Граф дорог местности\n",
        "\n",
        "    def __load_config(self, file_path: str) -> None:\n",
        "        \"\"\"Загрузка данных о константах через файл конфигурации.\n",
        "\n",
        "        Args:\n",
        "            file_path (str): Путь к файлу конфигурации.\n",
        "        \"\"\"\n",
        "\n",
        "        with open(file_path, \"r\") as file:\n",
        "            config = json.load(file)\n",
        "            self.__data_amount = config[\"data_amount\"]  # Размер генерируемой выборки\n",
        "            self.__min_segment = config[\n",
        "                \"min_segment\"\n",
        "            ]  # Минимальное значение отрезка для создания отклонения\n",
        "            self.__max_segment = config[\n",
        "                \"max_segment\"\n",
        "            ]  # Максимальное значение отрезка для создания отклонения\n",
        "            self.__min_offset = config[\"min_offset\"]  # Минимальное отклонение\n",
        "            self.__max_offset = config[\"max_offset\"]  # Максимальное отклонение\n",
        "            self.__max_route_len = config[\"max_route_len\"]\n",
        "            self.__min_route_len = config[\"min_route_len\"]\n",
        "\n",
        "    def save_false_route(self, main_route: list) -> tuple:\n",
        "        \"\"\"Генерация одного искажённого маршрута на основе исходного.\n",
        "\n",
        "        Args:\n",
        "            main_route: (list): Исходный маршрут.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[nx.Graph, list]: Кортеж, внутри которого помещён изменённый граф и полученный маршрут.\n",
        "        \"\"\"\n",
        "\n",
        "        path = main_route\n",
        "        G = self.graph.copy()\n",
        "        new_nodes = [path[0]]\n",
        "\n",
        "        for i in range(len(path) - 1):\n",
        "            # Начальная и конечная точки отрезка\n",
        "            u, v = path[i], path[i + 1]\n",
        "            point1 = (G.nodes[u][\"y\"], G.nodes[u][\"x\"])\n",
        "            point2 = (G.nodes[v][\"y\"], G.nodes[v][\"x\"])\n",
        "\n",
        "            # Расстояние между узлами\n",
        "            edge_length = gd(point1, point2).meters\n",
        "            direction_bearing = ox.bearing.calculate_bearing(\n",
        "                point1[0], point1[1], point2[0], point2[1]\n",
        "            )\n",
        "\n",
        "            # Добавление точек через случайное расстояние между 20 и 60 метров\n",
        "            current_dist = 0\n",
        "            previous_node = u\n",
        "            while current_dist < edge_length:\n",
        "                # Случайное расстояние до следующей точки\n",
        "                random_dist = random.uniform(self.__min_segment, self.__max_segment)\n",
        "                current_dist += random_dist\n",
        "\n",
        "                if current_dist >= edge_length:\n",
        "                    break\n",
        "\n",
        "                # Вычисление промежуточной точки\n",
        "                new_point = gd(meters=current_dist).destination(\n",
        "                    point1, direction_bearing\n",
        "                )\n",
        "                new_lat, new_lon = new_point.latitude, new_point.longitude\n",
        "\n",
        "                # Случайное отклонение влево или вправо\n",
        "                offset_direction = direction_bearing + (\n",
        "                    90 if random.choice([True, False]) else -90\n",
        "                )\n",
        "                offset_dist = random.uniform(self.__min_offset, self.__max_offset)\n",
        "                offset_point = gd(meters=offset_dist).destination(\n",
        "                    (new_lat, new_lon), offset_direction\n",
        "                )\n",
        "                offset_lat, offset_lon = (offset_point.latitude, offset_point.longitude)\n",
        "\n",
        "                # Добавление новой вершины и её координат\n",
        "                new_node = max(G.nodes) + 1\n",
        "                G.add_node(new_node, y=offset_lat, x=offset_lon)\n",
        "                new_nodes.append(new_node)\n",
        "\n",
        "                # Добавление ребра между новой точкой и предыдущей точкой\n",
        "                G.add_edge(previous_node, new_node, length=random_dist)\n",
        "                G.add_edge(\n",
        "                    new_node, v, length=edge_length - current_dist\n",
        "                )  # Связь с основным маршрутом\n",
        "\n",
        "                previous_node = new_node  # Сместить начальную точку для следующего шага\n",
        "            new_nodes.append(path[i + 1])\n",
        "\n",
        "        false_route = [(G.nodes[n][\"x\"], G.nodes[n][\"y\"]) for n in new_nodes]\n",
        "        return G, false_route\n",
        "\n",
        "    def save_main_route(self) -> tuple:\n",
        "        \"\"\"Генерация и сохранение исходного маршрута\n",
        "\n",
        "        Args: _\n",
        "        \"\"\"\n",
        "        keys = list(self.graph.nodes.keys()).copy()\n",
        "        node_ids = []\n",
        "\n",
        "        while len(node_ids) < self.__min_route_len or len(node_ids) > self.__max_route_len:\n",
        "            try:\n",
        "                start = random.choice(keys)\n",
        "                keys.remove(start)\n",
        "                end = random.choice(keys)\n",
        "                # Поиск кратчайшего пути\n",
        "                node_ids = nx.astar_path(self.graph, start, end, weight=\"length\")\n",
        "            except nx.NetworkXNoPath:\n",
        "                pass\n",
        "        main_route = [(self.graph.nodes[n][\"x\"], self.graph.nodes[n][\"y\"])\n",
        "                      for n in node_ids]\n",
        "        return node_ids, main_route\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_cumulative_distances(route: \"np.ndarray\"):\n",
        "        distances = [0]  # Начинаем с 0 элемента\n",
        "        for i in range(1, len(route)):\n",
        "            lon1, lat1 = route[i - 1]\n",
        "            lon2, lat2 = route[i]\n",
        "            distance = haversine((lon1, lat1), (lon2, lat2), unit=Unit.METERS)\n",
        "            distances.append(distances[-1] + distance)\n",
        "        return np.array(distances)\n",
        "\n",
        "    # Функция для интерполяции маршрута\n",
        "\n",
        "    def make_equal(self, route: list, num_points: int) -> list:\n",
        "        route = np.array(route)\n",
        "        # Вычисляем кумулятивное расстояние\n",
        "        distances = self.calculate_cumulative_distances(route)\n",
        "\n",
        "        # Создаем интерполяционные функции для широты и долготы\n",
        "        interpolation_func_lon = interp1d(distances, route[:, 0], kind='linear')\n",
        "        interpolation_func_lat = interp1d(distances, route[:, 1], kind='linear')\n",
        "\n",
        "        new_distances = np.linspace(0, distances[-1], num_points)\n",
        "\n",
        "        new_lon = interpolation_func_lon(new_distances)\n",
        "        new_lat = interpolation_func_lat(new_distances)\n",
        "        new_route = list(np.column_stack((new_lon, new_lat)))\n",
        "        new_route = [tuple(point) for point in new_route]\n",
        "        return new_route\n",
        "\n",
        "    def save_data(self) -> None:\n",
        "        for i in range(self.__data_amount):\n",
        "            route_ids, main_route = self.save_main_route()\n",
        "            _, false_route = self.save_false_route(route_ids)\n",
        "\n",
        "            main_route = self.make_equal(main_route, len(false_route))\n",
        "            self.data['y'].append(main_route)\n",
        "            self.data['X'].append(false_route)\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"Сделано {i + 1}/{self.__data_amount} маршрутов\")\n"
      ],
      "metadata": {
        "id": "6h9wt_KDgiAI",
        "ExecuteTime": {
          "end_time": "2025-04-01T20:40:59.628499Z",
          "start_time": "2025-04-01T20:40:59.602569Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Функции для запуска примера"
      ],
      "metadata": {
        "id": "u73_9yRs1KUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В качестве примера будут созданы 3 html-файла в папке example:\n",
        "- *input* - входной маршрут\n",
        "- *target* - целевой маршрут\n",
        "- *predict* - предсказанный моделью маршрут"
      ],
      "metadata": {
        "id": "SOrS10Ci4b3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_route(points: list, save_folder: str, name: str) -> None:\n",
        "    # Создаем карту, центрированную на первой точке\n",
        "    points = [(point[1], point[0]) for point in points]\n",
        "    plot = folium.Map(location=points[0], zoom_start=15)\n",
        "\n",
        "    # Соединяем точки линией (маршрут)\n",
        "    folium.PolyLine(points, color=\"red\", weight=2, opacity=1).add_to(plot)\n",
        "\n",
        "    # Сохраняем карту в HTML-файл и открываем его\n",
        "    plot.save(f\"{save_folder}/{name}.html\")\n",
        "\n",
        "\n",
        "def lstm_test(save_folder: str) -> None:\n",
        "    os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "    # test_model = torch.load('lstm_model.pth', weights_only=False)\n",
        "    test_model = CNN1d()\n",
        "    test_model.eval()\n",
        "\n",
        "    sc = MinMaxScaler(feature_range=(-1, 1))\n",
        "\n",
        "    place_bbox = [39.0296, 51.7806, 39.3414, 51.5301]\n",
        "\n",
        "    generator = RouteGenerator(place_bbox=place_bbox, config_path=\"configs\\\\config.json\")\n",
        "    G, result = generator.graph, generator.save_main_route()\n",
        "    main_ids, main_coords = result\n",
        "    G_false, false_coords = generator.save_false_route(main_ids)\n",
        "\n",
        "    main_coords = generator.make_equal(main_coords, len(false_coords))\n",
        "\n",
        "    save_route(main_coords, save_folder, \"target\")\n",
        "    save_route(false_coords, save_folder, \"input\")\n",
        "\n",
        "    false_coords = torch.tensor(sc.fit_transform(false_coords), dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        predict = test_model(false_coords)\n",
        "    predict = sc.inverse_transform(predict.detach().numpy())\n",
        "    print(predict)\n",
        "    save_route(predict, save_folder, \"predict\")\n",
        "    print(f\"MSE: {mean_squared_error(predict, main_coords)} \\t MAE: {mean_absolute_error(predict, main_coords)}\")\n"
      ],
      "metadata": {
        "id": "1KgyHYtKgVDI",
        "ExecuteTime": {
          "end_time": "2025-04-01T20:42:26.699063Z",
          "start_time": "2025-04-01T20:42:26.690794Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Тестирование RNN"
      ],
      "metadata": {
        "id": "q4jUyVBLW1Ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Тестирование 1dCNN"
      ],
      "metadata": {
        "id": "7KjjSJnuW34i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Тестирование ансамблей"
      ],
      "metadata": {
        "id": "ljRCd37_W6zO"
      }
    }
  ]
}